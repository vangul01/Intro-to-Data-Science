{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature extractions\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. One-hot encoding\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "features = [ {'city': 'Dubai', 'temperature': 33.}, \n",
    "        {'city': 'London', 'temperature': 12.}, \n",
    "        {'city': 'San Francisco', 'temperature': 18.}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  vec.fit_transform(features)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try: one-hot encode HW2's dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag-of-words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['This is the first document.',\n",
    "'This document is the second document.',\n",
    "'And this is the third one.',\n",
    "'Is this the first document?']\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(corpus)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twogram_vec = CountVectorizer(ngram_range=(1,2))\n",
    "X = twogram_vec.fit_transform(corpus)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twogram_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vec.fit_transform(corpus)\n",
    "X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.vocabulary_.get('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.transform(['Something completely new document.']).toarray()\n",
    "print(vec.transform(['Something completely and.']).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature selections\n",
    "https://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 regularization gives sparse solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Fabian Pedregosa <fabian.pedregosa@inria.fr>\n",
    "#         Alexandre Gramfort <alexandre.gramfort@inria.fr>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import datasets\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "print(\"Computing regularization path using the LARS ...\")\n",
    "_, _, coefs = linear_model.lars_path(X, y, method='lasso', verbose=True)\n",
    "\n",
    "xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "xx /= xx[-1]\n",
    "\n",
    "plt.plot(xx, coefs.T)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(xx, ymin, ymax, linestyle='dashed')\n",
    "plt.xlabel('|coef| / max|coef|')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('LASSO Path')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tree based feature selections\n",
    "The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features. In scikit-learn, the fraction of samples a feature contributes to is combined with the decrease in impurity from splitting them to create a normalized estimate of the predictive power of that feature.\n",
    "\n",
    "By averaging the estimates of predictive ability over several randomized trees one can reduce the variance of such an estimate and use it for feature selection. This is known as the mean decrease in impurity, or MDI. Refer to [L2014] for more information on MDI and feature importance evaluation with Random Forests. (https://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "%matplotlib inline\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Number of cores to use to perform parallel fitting of the forest model\n",
    "n_jobs = 1\n",
    "\n",
    "# Load the faces dataset\n",
    "data = fetch_olivetti_faces()\n",
    "X = data.images.reshape((len(data.images), -1))\n",
    "y = data.target\n",
    "\n",
    "mask = y < 5  # Limit to 5 classes\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# Build a forest and compute the pixel importances\n",
    "print(\"Fitting ExtraTreesClassifier on faces data with %d cores...\" % n_jobs)\n",
    "t0 = time()\n",
    "forest = ExtraTreesClassifier(n_estimators=1000,\n",
    "                              max_features=128,\n",
    "                              n_jobs=n_jobs,\n",
    "                              random_state=0)\n",
    "\n",
    "forest.fit(X, y)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "importances = forest.feature_importances_\n",
    "importances = importances.reshape(data.images[0].shape)\n",
    "\n",
    "# Plot pixel importances\n",
    "plt.matshow(importances, cmap=plt.cm.hot)\n",
    "plt.title(\"Pixel importances with forests of trees\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suggested reading: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html <br>\n",
    "References: From the official documentations of scikit learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
